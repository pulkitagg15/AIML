{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f169cc4",
   "metadata": {},
   "source": [
    "### LangChain: Memory\n",
    "### Outline\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bf443",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b922c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "load_dotenv(dotenv_path=\"/Users/pulkitaggarwal/LangChain1/Gemini_Key.env\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"Hello Gemini\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab32c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c03595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create memory and chain\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd92e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi Andrew! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine with the ability to hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556ac9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine with the ability to hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1 + 1 = 2.  That's a pretty basic arithmetic problem!  My training data includes a vast amount of mathematical information, so I can easily handle that calculation.  Is there anything else I can help you with? Perhaps a more challenging math problem, or something completely different?  I'm ready for whatever you throw at me!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3307b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine with the ability to hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 = 2.  That's a pretty basic arithmetic problem!  My training data includes a vast amount of mathematical information, so I can easily handle that calculation.  Is there anything else I can help you with? Perhaps a more challenging math problem, or something completely different?  I'm ready for whatever you throw at me!\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Andrew.  You told me that at the beginning of our conversation. I have a good memory (within the constraints of my current session, of course!), and I'm able to retain information from our interaction.  Is there anything else I can help you with today?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea70853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine with the ability to hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 = 2.  That's a pretty basic arithmetic problem!  My training data includes a vast amount of mathematical information, so I can easily handle that calculation.  Is there anything else I can help you with? Perhaps a more challenging math problem, or something completely different?  I'm ready for whatever you throw at me!\n",
      "Human: What is my name?\n",
      "AI: Your name is Andrew.  You told me that at the beginning of our conversation. I have a good memory (within the constraints of my current session, of course!), and I'm able to retain information from our interaction.  Is there anything else I can help you with today?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d08045b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi, my name is Andrew\\nAI: Hi Andrew! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine with the ability to hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?\\nHuman: What is 1+1?\\nAI: 1 + 1 = 2.  That\\'s a pretty basic arithmetic problem!  My training data includes a vast amount of mathematical information, so I can easily handle that calculation.  Is there anything else I can help you with? Perhaps a more challenging math problem, or something completely different?  I\\'m ready for whatever you throw at me!\\nHuman: What is my name?\\nAI: Your name is Andrew.  You told me that at the beginning of our conversation. I have a good memory (within the constraints of my current session, of course!), and I\\'m able to retain information from our interaction.  Is there anything else I can help you with today?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d99cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e2e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a159f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53f42d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be913620",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b9fa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e2b60",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6944e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f08732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84700e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97712b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9abb48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create memory and chain\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9a2466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Andrew! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine with the ability to hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7530fd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1 + 1 = 2.  That's a pretty basic arithmetic problem!  My training data includes a vast amount of mathematical information, so I can easily handle that calculation.  Is there anything else I can help you with? Perhaps a more complex equation, or something completely different?  I'm ready for your next question!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7a4e992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Andrew.  We established that at the beginning of our conversation.  I have a good memory (within the context of our current interaction, of course!). Is there anything else I can assist you with?  Perhaps you'd like to explore a topic related to mathematics, history, or maybe even the intricacies of large language models?  I'm eager to continue our chat!\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf9fe3",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "185604ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72ab717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "# Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Token-aware memory (based on max token count)\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=1000  # adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6707a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ea3efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf749bf",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdde9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adac685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/hxgvmlms6j39627ykffdygcm0000gn/T/ipykernel_57672/1387621936.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
     ]
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "695d6209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI, and after some casual exchange, asks what is on the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6880a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cf81261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and after some casual exchange, asks what is on the schedule for the day.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a great question!  Since your customer is driving over an hour to learn about the latest in AI, I'd recommend focusing on a demo that showcases the practical applications of LLMs and highlights LangChain's capabilities.  Avoid anything too technical or theoretical.  Here are a few options, ranked in order of my recommendation:\\n\\n1. **A customized question-answering system using LangChain and an LLM:** This is impactful because it directly demonstrates the power of LLMs for information retrieval and processing. You could pre-load it with information relevant to your customer's industry or business needs.  For example, if they're in finance, you could have it answer questions about recent market trends or regulations.  The demo would involve you typing in a question, and the system providing a concise and accurate answer, highlighting LangChain's role in managing the process.\\n\\n2. **A simple text summarization demo:**  This is easy to understand and visually appealing.  You could feed it a longer document (perhaps a news article or a report relevant to their industry) and have it generate a concise summary. This showcases the LLM's ability to process and condense information, a valuable skill in today's data-rich environment.  Again, emphasize LangChain's role in streamlining the process.\\n\\n3. **A chatbot interaction (with limitations clearly stated):**  While chatbots are common, you can make this demo stand out by focusing on a specific, relevant use case.  For example, you could create a chatbot that answers frequently asked questions about your product or service.  Crucially, you should be upfront about the chatbot's limitations – it's not perfect, and it might not be able to answer every question.  This demonstrates honesty and manages expectations.\\n\\n**What to avoid:**\\n\\n* **Anything too abstract or theoretical:**  Focus on concrete applications and tangible results.\\n* **Overly complex demos:**  Keep it simple and easy to understand, even for someone without a deep technical background.\\n* **Demos that rely on unstable internet connections:**  Ensure everything runs smoothly offline or with a reliable connection.\\n\\nRemember to tailor the demo to your customer's specific needs and interests.  Before the meeting, try to learn as much as possible about their company and their challenges.  This will allow you to choose the most relevant and impactful demo.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af04d19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI and inquires about the day's schedule. The AI outlines a busy day including an 8am product team meeting requiring a PowerPoint presentation, followed by LangChain project work (9am-12pm), and a 12pm lunch meeting with a customer driving over an hour for an AI demonstration.  The human asks for demo suggestions, and the AI recommends three options ranked by impact: a customized question-answering system using LangChain and an LLM, a text summarization demo, and a chatbot interaction (with limitations clearly stated). The AI emphasizes focusing on practical applications, avoiding overly complex or theoretical demos, and tailoring the chosen demo to the customer's specific needs and interests.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ddb31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
